services:
  vllm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    ports:
      - "8000:8000"
    # Даем контейнеру доступ к GPU
    gpus: all
    environment:
      # Для отладки vLLM-логов, если понадобится
      VLLM_LOGGING_LEVEL: info

  bot:
    build:
      context: .
      dockerfile: Dockerfile.bot
    # Ждём, пока vllm не будет запущен
    depends_on:
      - vllm
    # Подключаем секреты и ключи
    env_file:
      - .env
    # Перенаправляем OpenAI-запросы на наш vllm-контейнер
    environment:
      OPENAI_API_BASE: http://localhost:8000/v1