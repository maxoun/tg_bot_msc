# 1. Модель (последовательный аргумент "model" конфиг-файла)
model: Qwen/Qwen3-8B-AWQ      # идентификатор в HF Hub или локальный путь :contentReference[oaicite:0]{index=0}

# 2. HTTP‑сервер (OpenAI‑совместимый API)
host: 0.0.0.0                 # слушаем на всех интерфейсах :contentReference[oaicite:1]{index=1}
port: 8000                    # порт для запросов :contentReference[oaicite:2]{index=2}

# 3. Параметры движка (engine_args)
gpu-memory-utilization: 0.6   # VRAM под KV‑кеш, без offload на CPU :contentReference[oaicite:3]{index=3}  
max-model-len: 6192           # макс. длина контекста (Prompt+Output) 6192 токенов :contentReference[oaicite:4]{index=4}  
tensor-parallel-size: 1       # одиночный-GPU шардирование весов   
pipeline-parallel-size: 1     # pipeline‑параллелизм между узлами   
block-size: 32                # размер блока KV‑кеша (токенов), максимум на CUDA   

# 4. API‑сервер
api-server-count: 1           # число Uvicorn‑воркеров   
uvicorn-log-level: "info"     # уровень логов Uvicorn :contentReference[oaicite:9]{index=9}
